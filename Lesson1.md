# 简介
1. 大模型从专用模型向通用大模型发展，书生浦语大模型成为研究和工业应用的热点。
2. 书生浦语大模型的开源历程，包括不同尺寸和类型的模型，如7b和20b，以及不同模型的特点和应用场景。
3. 书生浦语的全链条开源体系涵盖了数据、预训练、微调、部署、评测和应用等环节，为开发者提供了丰富的工具和资源。

# 1.大模型成为发展通用人工智能的重要途径
一个模型来应对多种任务和多种模态，去解决多种跨模态任务。通用大模型是通往通用人工智能的一个就关键的途径。

# 2.书生浦语大模型
## 2.1书生浦语大模型的开源历程
2023年6月份，第一次发布
2023年7月份，**千亿参数的大模型进行了全面的升级**-支持了8k语境，26种语言，**全面开源，免费商用**7b开源模型和全链条的工具体系
2023年8月份，**书生·万卷1.0**——多模态的预训练语料库；**升级版对话模型**——开源的智能体的框架支持；**123b千亿参数模型**
2023年9月份，增强版20B开源模型，升级开源工具链
2024年1月份，InternLM2正式开源

## 2.2InternLM2 的体系
轻量级7b和重量级20b都各包含三个模型：
InternLM2-Base
高质量和具有很强可塑性的模型基座，是进行深度领域适配的高质量起点
InternLM2：多个能力方向强化
InternLM2-Chat
在Base基础上，经过SFT和RLHF，面向对话交互进行了优化，有很好的指令理解、共情聊天和调用工具等能力

## 2.3 回归语言建模的本质
关键点：**高质量语料**，让模型能够学会就更好的建模能力
### 新一代的数据清洗过滤技术
多维度的数据价值评估
高质量语料驱动的数据富集
数据的补齐

## 2.4 书生浦语2.0的亮点
1. 具备超长的上下文的能力，支持了20万token上的一个长度，也就是200k它可以实现比较完美的大海捞针的测试
2. 综合性能也得到了全面提升。包括像推理数学代码等等
3. 优秀的对话和创作体验，像指令遵循的能力，然后结构化创作能力都是非常不错
4. 工具调用的能力整体的升级，可以去做可靠的支持，工具多人调用，能够去搭建复杂的智能体
5. 突出的数据能力和使用的数据分析的功能，强化了它的内生计算能力，通过加入代码解释器，在特定评测集上，可以达到和GP4相仿的水平

### 应用举例
ai助手
充满人文关怀的对话
复杂这种数学题的运算和求解

## 2.5 模型到应用典型流程
1. 模型的选型，是模型评测过程
   社区各个不同的开源模型，在一些经典的评测集上，或者大规模的榜单上，整体效果，选其中一个，或几个在业务场景里考虑
2. 考虑业务场景是否复杂
   是比较复杂业务场景：我们就可能去需要去对模型进行微调，使其具备一些业务场景的相关的一些知识
   1）算力足够。全参数的微调
   2）算力有限。部分参数微调，比如Lora微调模型
3. 评估模型是否需要和环境去做交互
   在业务系统里面，或实际应用里面，是否有这种外部的API或者说外部一些工具，需要模型去做交互
   1）如果需要的话，我们需要去进一步构建智能体
   2）如果不需要，那就可以啊就直接去考虑测试模型性能
3. 上线之前进行模型评测
   在实际的场景里面，或者或者说我们实际的这种叫预期的一些领域里面，然后对它做进进一步的评测。
4. 模型部署和上线

# 3 书生浦语全链条开源开放体系
## 3.1 数据
书生万卷1.0：多模态的数据集，总数据量其实达到了两个tb，并里面包括有文本数据，有图像文本的数据，有视频数据，
书生万卷CC：它包括了从2013年到2023年的互联网的公开内容，它是基于CC这样的一个数据，做了非常精细的清洗，并就安全上面也做了处理。
大家都可以在open data lab这样的平台，相当于是实验室的数据平台上来去获取到包括这边的数据集，以及说还有很多其他的一些社区的开源数据集。

## 3.2 预训练
我们也开就相当于开发了 interview这样的一个预训练的框架，且它具备这种高可扩展性，就支持了从8卡到千卡级的一个训练，并且在千卡的级别上面加速效率达到了92%，是远远超过了比如说啊一些竞品，就竞品的框架，同时的话通过实验室这边的一些优化技术，来能够去实现性能的一个技术优化，来达到50%的加速，同时也能够去兼容主流的一些技术生态，比如说像根face的模型和各种轻量化的技术，并且能够开箱即用，也支持了多种不同规格的语言模型，只需要修改配置，就可以去进行模型的预训练。

## 3.3 微调-X-tuner微调框架
1. 增量续训
  类似于训练的方式，让模型学到一些新的知识，比如某个垂类领域的知识。
  那这边的训练数据可能是比如说像书籍、文章、代码等等

2. 有监督的微调，主要是让模型学会理解各种指令来去进行对话，或者我们也可以通过有监督微调方式去注入少量领域知识。那这边的训练数据通常来讲是高质量的这种对话或者问答的数据。
  1）全量参数的微调，也就是说我们所模型所有的参数都是放开训练的
  2）部分参数的微调，比如说Lora，固定模型的绝大部分参数，然后通过放开或者引入少量的参数，然后这样能能够去降低整体的一个微调的成本。
那这边我们有高效微调框架出纳，它能够去适配不同的各种各样的生态，比如说不同的一些微调策略和算法，能够去覆盖各类的sft的场景，同时的话它也能够去适配不同的一些开源生态，比如说加载它跟face的模型， model scope的一些模型或者说数据集，同时里面也会去做自动的优化加速优化加速，让开发者可以不用去关注这些复杂的一些显存优化和计算加速的细节，可以把更多的一些精力来去投入到比如说数据的一些准备和优化上面。

3. X-tuner
   能够适配不同的硬件，能够覆盖比如说像Nvidia20系列以上的所有显卡，最低其实只需要8G的显存就可以去微调7b的模型。


## 3.4 评测_open compass2.0司南大模型评测体系
1. 中立全面的性能榜单，就包括就大语言模型的这种榜单，包括多模态模型评测的榜单，那基本上嗯就主流的模型大家应该都可以在这个榜单上面来看到，我们也会定期的去发布相关的一些更新，以及说还有capacity就大模型的评测的权重工具链，我们会把就评测所依赖的各种相关的工具，其实都在open卡帕斯这样的一个get up，我刚才在心里面去做一个开源，那里面会包括就除了各个数据集的兼容，目前里面应该已经支持了超过100个数据集的评测。

，那它包括也也有很丰富的这种模型推理的接入，然后也包括像数据污染检查的一些功能，长文本能力评测，以及说中英文双语的一些主观评测，能够去覆盖就多个维度多种需求的这种评测。同时的话我们也建立了卡帕萨这样的一个高质量评测，基准的一个社区，我们是希望以开源开放共建共享的方式来去构建大模型的评测基准，或者说评测级的社区来去让更多的一些研究者和开发者参与到大模型的评测中来，来去把各种优秀的这种评测集能够在社区去进行汇聚，同时也提供更好的一些这种支撑的服务。

那 Open卡pass的话，其实它现在已经有非常广泛的应用，很多的一些大模型的这种头部的企业或者说研究机构，他们在发布大模发布模型的时候都会用open car pass来去进行评测，那同时的话它也是获得麦塔官方推荐的唯一的一个国产大模型的评测体系，目前也是社区支持最完善的评测体系之一，应该已经适配了超过100个评测集，50多万道题目，并且啊就整个的一个套评测工具和评包括评测的一些prompt，其实都是开源的。那我们从就整体的open卡pass2.0，我们自己构建的一些评测群里面也可以发现一些趋势，我们对啊主流的一些啊模型都做了一些评测的分析

### 评测结论
当然的话我们我们不不太需要去关注具体这边每个模型它的一个性能，因为我们更多的是希望去给社区提供一些这种关于模型整体的发展的一些趋势的了解。那首先第一个这边是我们有一个综合性的客观评测，也就是说在我们自己构造的客观的评测集上来去进行啊就结果的评测。这边会从语言知识推理、数学代码、智能体这多个方面来去进行评测。就左边最高的是GP4，那我们也可以看到，其实整体能力大家是有比较大的提升空间的。
因为在open campus2.0里面，我们采用了更加准确的这种循环评测的策略，之前的很多评测集其实是采用这种选择题的模式，比如说 ABCD然后让模型去选择，那这样的话模型其实有一些这种猜测的因素，或者说一些随机就随机的因素在那在open卡pass2.0里面，我们采用了循环评测的策略，也就是说我们会把选项去进行一个这种轮换，模型只有在能够答对所有这种轮换的方式的时候，我们才认为模型这道题是对的，那这样的话其实就能够获得更加准确的一个性能。那我们发现就哪怕是GT4，在这样的一个百分制的评测基准里面，也只是达到了61.8分的一个大概及格的水平。
但如果是一些比较复杂的推理场景，那有可能差距就显现的非常大。同时我们也发现就这种偏理科的能力的一些啊分数和模型的尺寸有比较强的一个关联性。比如说像语言和知识这样的一类文偏文科的维度上，中轻量级的模型和这种重量级，或者说必然的商业模型差距比较小。但是再比如说像数学推理代码这样的一一些偏理科的维度上，性能和尺寸就呈现比较强的相关性。同时的话我们也通过这种主客观的评测，其实也发现有些模型可能在主观和客观的性能上存在比较大的偏差，所以社区不仅是需要去夯实这种客观评测的能力基础，也需要在一些对话体验上来去下功夫。
那这边的话其实就是主观评测的一个胜率，我们是让就对比，比如说每个不同的模型和GP4的一个这种综合的一个胜率，那 GP4的胜率就有50%，那我们也可以可以看到 GP4在主观评测里面仍然现在是占据着第一的位置。那我们也可以看到就国内近期发布的部分的大模型其实表现是非常优异的，多个维度上缩小了和GPS Top的一些差距，并且国内的模型在中文场景下是具有性能优势。再比如说像中文的语言理解中文知识，还有中文的创作上，国内的商业模型相比于GPS淘宝，其实是具有很强的竞争能力的，甚至有些少数模型它在单个维度上，甚至能够去完成对GPS Top的超越。

## 3.5 模型部署
模型部署通常来讲是模型上线应用里面的最后一环，那怎么样去把模型能够去就在GPU上更快的跑起来，然后去得到更极致的性能显存等等的一些优化。那这里面MD泡泡其实做了非常多的一些工作，那它里面支持了包括模型的轻量化，比如说4比特权重的量化，然后8比特KB开始的量化，然后也知我们也开发了不同的推力引擎，比如说像Top one的，或者说基于排套的推力引擎，同时的话我们也啊就开发了这样的一些服务的模块，比如说能够去兼容open AI的这这种server的接口，然后也有grade的一些这种啊这个 word demo，就各种各样的一些，比如说它需要去部署上线，所依赖的一些这种功能或者feature，很多在mdp里面都可以找到，那同时我们也提供了不同的一些访问接口，比如说像直接的Python接口，或者说像这种rapper的接口和gipc的接口，那这里面的话我们其实就在进行了一些高效的推力引擎的优化，比如说有这种持续批处理的一些技巧，然后也有一些深度优化的这种低比特计算的一些卡诺，那还有就是说关于模型并行的一些优化和这种cave开始的一些管管理的机制。
那就 Md破它其实也是一个就比较完备的一个工具链，他能够去完成从量化到推理到服务这样的一个全流程，也可以无缝的对接 open capacity来去评测推理的精度。那实验室内部现在也是会使用 MD point和我们哪怕是去做这种评测的一个加速，我们也可以去这边也有多维度的推理速度的评测的工具，同时的话MD破也支持这种交互式的推理，比如说类似于啊我们啊就跟类似于UI上面，我们可能问一句答一句这种那也支持，比如说这种非交互式的，我们把整个对话历史都发给模型，那这里的话是MD破e和VR m这样的一个社区非常流行的推理框架，它的一个性能的对比。

## 3.6 智能体

我们可以看到在a版上， m deploy它的一个推理效率是要在不同的模型尺寸上其实都是优于VR m的，那这这样的话其实也能够为我们的线上服务，留就相当于预留更多的一个容量，然后也能够让我们的一些这种推荐人的响应能够更更快。那最后也介绍一下我们智能体的框架，比如说这边我们开发了一个轻量级的智能体框架Legend，那它只能够去支持多种的智能体的能力，比如说像react的一些啊还有人物，还有奥特GP，这样的不同的一些这种智能体的一些派派，比如说我们啊从输入，然后接下来怎么去选择选择和执行工具，以及说是否有计划拆拆分等等，那这样不同的一些智能体的流程，其实都是在里面可以去一键支持，以及说可以去开发新的流程，并且也可以去灵活的支持各种大语言模型，包括一些这种大模型的API像GP3.5和GP4，那也支持比如说一些开模型，像它跟face里面的模型，还有英特尔的模型，它也能够有很多的一些内置的工具，可以去直接使用。
那这边的话比如说也举两个例子，就一个是通过内镜的这样的一个框架，我们就可以让模型去实现。比如说代码刚才提到的代码解数学题的能力，以及是说这边还有一个多模态AI工具的使用，我们可以做到零样本的一个泛化，那这样的话其实这个 demo也是基于 latent，我们可以去搭建，那智能体的框架其实就给我们如何去使用和进一步的去开发大模型，就留下了很多的一些这种可能性。那刚才其实雷军的是这种智能体的整体的一个框架，那除此之外我们还开发了多媒体多模态智能体的一个工具箱啊就 agent Lego。
这边的话其实它主要是一个比较丰富的工具集合，尤其是它提供了大量的这种跟视觉多模态相关的领域的一些前沿的算法功能。比如说啊就open map是目前计算机视觉领域应该是最大或者说最全的一个模型库，那我们其实也把open map里面的很多的典型的算法，或者说高性能的算法，其实在这里面去做了封装。那我们除了除此之外，还包括像stable diffusion，然后像一些语音的模型，包括像这种Sam这一类分割的模型，在agent legal里面其实都做了支持。所以的话当我们想要去把就语言模型去拓展到一个这种多模态智能体的时候，我们可以借助h and echo这样的一个就工具箱来去完成整体的一个这种啊智能体的一个啊就工具集，然后大家就可以focus在智能体本身的一些开发上。

那 A7Nike的话，它其实能够去兼容，雷军也可以去兼容，比如说像狼称和创新购买agents就其他的一些这种agent的框架。所以的话我们其实是把智就智能体的工具箱和智能体本身框架就框架本身是也是做了一定的解耦。好，那最后的话来回顾一下，就是说书生浦语的全链条开源就开源开放体系，包括了从数据到预训练到微调到部署到评测到应用，这样的不同的一些环节，就整体大家需要去基于大模型，然后去做这样的各种各样的一些应用开发和一些微调实现，基本上都可以在英特尔这样的一个就开源的工具链里面去找到相应的工具。













